{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from pandas import DataFrame\n",
    "from pandas import Series\n",
    "from pandas import concat\n",
    "from pandas import read_csv\n",
    "from pandas import datetime\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import GRU\n",
    "from keras.layers import RNN\n",
    "\n",
    "from math import sqrt\n",
    "from matplotlib import pyplot\n",
    "import numpy\n",
    "\n",
    "# date-time parsing function for loading the dataset\n",
    "def parser(x):\n",
    "    return datetime.strptime('190'+x, '%Y-%m')\n",
    "\n",
    "# frame a sequence as a supervised learning problem\n",
    "def timeseries_to_supervised(data, lag=1):\n",
    "    df = DataFrame(data)\n",
    "    columns = [df.shift(i) for i in range(1, lag+1)]\n",
    "    columns.append(df)\n",
    "    df = concat(columns, axis=1)\n",
    "    df.fillna(0, inplace=True)\n",
    "    return df\n",
    "\n",
    "# create a differenced series\n",
    "def difference(dataset, interval=1):\n",
    "    diff = list()\n",
    "    for i in range(interval, len(dataset)):\n",
    "        value = dataset[i] - dataset[i - interval]\n",
    "        diff.append(value)\n",
    "    return Series(diff)\n",
    "\n",
    "# invert differenced value\n",
    "def inverse_difference(history, yhat, interval=1):\n",
    "    return yhat + history[-interval]\n",
    "\n",
    "# scale train and test data to [-1, 1]\n",
    "def scale(train, test):\n",
    "    # fit scaler\n",
    "    scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "    scaler = scaler.fit(train)\n",
    "    # transform train\n",
    "    train = train.reshape(train.shape[0], train.shape[1])\n",
    "    train_scaled = scaler.transform(train)\n",
    "    # transform test\n",
    "    test = test.reshape(test.shape[0], test.shape[1])\n",
    "    test_scaled = scaler.transform(test)\n",
    "    return scaler, train_scaled, test_scaled\n",
    "\n",
    "# inverse scaling for a forecasted value\n",
    "def invert_scale(scaler, X, value):\n",
    "    new_row = [x for x in X] + [value]\n",
    "    array = numpy.array(new_row)\n",
    "    array = array.reshape(1, len(array))\n",
    "    inverted = scaler.inverse_transform(array)\n",
    "    return inverted[0, -1]\n",
    "\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "    X, y = train[:, 0:-1], train[:, -1]\n",
    "\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "    print(\"X\", X)\n",
    "    model = Sequential()\n",
    "    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['mse', 'accuracy'])\n",
    "    for i in range(nb_epoch):\n",
    "        model.fit(X, y, epochs=1, batch_size=batch_size, verbose=0, shuffle=False)\n",
    "        model.reset_states()\n",
    "        print(model.evaluate())\n",
    "    return model\n",
    "\n",
    "# make a one-step forecast\n",
    "def forecast_lstm(model, batch_size, X):\n",
    "    X = X.reshape(1, 1, len(X))\n",
    "    yhat = model.predict(X, batch_size=batch_size)\n",
    "    return yhat[0,0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[   0. , -120.1],\n",
       "       [-120.1,   37.2],\n",
       "       [  37.2,  -63.8],\n",
       "       [ -63.8,   61. ],\n",
       "       [  61. ,  -11.8],\n",
       "       [ -11.8,   63.3],\n",
       "       [  63.3,   -7.3],\n",
       "       [  -7.3,  -31.7],\n",
       "       [ -31.7,  -69.9],\n",
       "       [ -69.9,  213.6],\n",
       "       [ 213.6, -150.6],\n",
       "       [-150.6,    8.4],\n",
       "       [   8.4,  -44.8],\n",
       "       [ -44.8,   60.6],\n",
       "       [  60.6,   63.2],\n",
       "       [  63.2,  -81.9],\n",
       "       [ -81.9,   95.6],\n",
       "       [  95.6,  -61. ],\n",
       "       [ -61. ,   77.6],\n",
       "       [  77.6,  -13.7],\n",
       "       [ -13.7,  131.7],\n",
       "       [ 131.7, -157.1],\n",
       "       [-157.1,   77.8]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# load dataset\n",
    "series = read_csv('shampoo.csv', header=0, parse_dates=[0], index_col=0, squeeze=True, date_parser=parser)\n",
    "\n",
    "# transform data to be stationary\n",
    "raw_values = series.values\n",
    "diff_values = difference(raw_values, 1)\n",
    "\n",
    "# transform data to be supervised learning\n",
    "supervised = timeseries_to_supervised(diff_values, 1)\n",
    "supervised_values = supervised.values\n",
    "\n",
    "# split data into train and test-sets\n",
    "train, test = supervised_values[0:-12], supervised_values[-12:]\n",
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.15241435, -0.80037766],\n",
       "       [-0.80037766,  0.04828702],\n",
       "       [ 0.04828702, -0.496628  ],\n",
       "       [-0.496628  ,  0.17669274],\n",
       "       [ 0.17669274, -0.21607769],\n",
       "       [-0.21607769,  0.1891017 ],\n",
       "       [ 0.1891017 , -0.1917993 ],\n",
       "       [-0.1917993 , -0.32344214],\n",
       "       [-0.32344214, -0.52953871],\n",
       "       [-0.52953871,  1.        ],\n",
       "       [ 1.        , -0.96493121],\n",
       "       [-0.96493121, -0.10709469],\n",
       "       [-0.10709469, -0.39411923],\n",
       "       [-0.39411923,  0.17453466],\n",
       "       [ 0.17453466,  0.18856218],\n",
       "       [ 0.18856218, -0.59428109],\n",
       "       [-0.59428109,  0.3633666 ],\n",
       "       [ 0.3633666 , -0.48152145],\n",
       "       [-0.48152145,  0.26625303],\n",
       "       [ 0.26625303, -0.22632857],\n",
       "       [-0.22632857,  0.55813326],\n",
       "       [ 0.55813326, -1.        ],\n",
       "       [-1.        ,  0.26733207]])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# transform the scale of the data\n",
    "scaler, train_scaled, test_scaled = scale(train, test)\n",
    "train_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.callbacks import Callback\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "import logging \n",
    "\n",
    "import keras\n",
    "from sklearn.metrics import roc_auc_score\n",
    " \n",
    "class ResetState(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        pass\n",
    " \n",
    "    def on_train_end(self, logs={}):\n",
    "        pass\n",
    " \n",
    "    def on_epoch_begin(self, epoch, logs={}):\n",
    "        return\n",
    " \n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        pass\n",
    " \n",
    "    def on_batch_begin(self, batch, logs={}):\n",
    "        pass\n",
    " \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.model.reset_states()\n",
    "#         print(\"reset model state\")\n",
    "        return\n",
    "    \n",
    "earlystop = EarlyStopping(monitor='val_loss',\n",
    "                          patience=1000,\n",
    "                          verbose=1,\n",
    "                          mode='auto')\n",
    "reset_state = ResetState()\n",
    "# fit an LSTM network to training data\n",
    "def fit_lstm(train, batch_size, nb_epoch, neurons):\n",
    "    epoch = 0\n",
    "    callbacks = [\n",
    "        earlystop,\n",
    "        reset_state\n",
    "    ]\n",
    "    X, y = train[:, 0:-1], train[:, -1]\n",
    "\n",
    "    X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "#     print(\"X\", X)\n",
    "    model = Sequential()\n",
    "                                                # batch size, time_steps, data_dim\n",
    "    model.add(LSTM(neurons, batch_input_shape=(batch_size, X.shape[1], X.shape[2]), stateful=True))\n",
    "    model.add(Dense(1))\n",
    "    model.compile(loss='mean_squared_error', optimizer='adam', metrics=['acc', 'mse' ])\n",
    "#     while earlystop.stopped_epoch == 0:\n",
    "    model.fit(X, y, epochs=nb_epoch, batch_size=batch_size, verbose=1, shuffle=False,  \n",
    "              callbacks=callbacks)\n",
    "#         print(\"epoch\", epoch,  model.history.history.get(\"loss\")[-1])\n",
    "#         epoch += 1\n",
    "    return model\n",
    "\n",
    "X, y = train[:, 0:-1], train[:, -1]\n",
    "\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "X.shape[1], X.shape[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23 1\n",
      "[[   0. ]\n",
      " [-120.1]\n",
      " [  37.2]\n",
      " [ -63.8]\n",
      " [  61. ]\n",
      " [ -11.8]\n",
      " [  63.3]\n",
      " [  -7.3]\n",
      " [ -31.7]\n",
      " [ -69.9]\n",
      " [ 213.6]\n",
      " [-150.6]\n",
      " [   8.4]\n",
      " [ -44.8]\n",
      " [  60.6]\n",
      " [  63.2]\n",
      " [ -81.9]\n",
      " [  95.6]\n",
      " [ -61. ]\n",
      " [  77.6]\n",
      " [ -13.7]\n",
      " [ 131.7]\n",
      " [-157.1]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[[   0. ]],\n",
       "\n",
       "       [[-120.1]],\n",
       "\n",
       "       [[  37.2]],\n",
       "\n",
       "       [[ -63.8]],\n",
       "\n",
       "       [[  61. ]],\n",
       "\n",
       "       [[ -11.8]],\n",
       "\n",
       "       [[  63.3]],\n",
       "\n",
       "       [[  -7.3]],\n",
       "\n",
       "       [[ -31.7]],\n",
       "\n",
       "       [[ -69.9]],\n",
       "\n",
       "       [[ 213.6]],\n",
       "\n",
       "       [[-150.6]],\n",
       "\n",
       "       [[   8.4]],\n",
       "\n",
       "       [[ -44.8]],\n",
       "\n",
       "       [[  60.6]],\n",
       "\n",
       "       [[  63.2]],\n",
       "\n",
       "       [[ -81.9]],\n",
       "\n",
       "       [[  95.6]],\n",
       "\n",
       "       [[ -61. ]],\n",
       "\n",
       "       [[  77.6]],\n",
       "\n",
       "       [[ -13.7]],\n",
       "\n",
       "       [[ 131.7]],\n",
       "\n",
       "       [[-157.1]]])"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = train[:, 0:-1], train[:, -1]\n",
    "print(X.shape[0], X.shape[1])\n",
    "print(X)\n",
    "X = X.reshape(X.shape[0], 1, X.shape[1])\n",
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0.23742304, 0.52194116, 0.72264266],\n",
       "        [0.44935059, 0.73673709, 0.06990145]],\n",
       "\n",
       "       [[0.69440555, 0.22356274, 0.8782029 ],\n",
       "        [0.2070233 , 0.19340162, 0.51538323]],\n",
       "\n",
       "       [[0.61229086, 0.91805906, 0.16913388],\n",
       "        [0.25195517, 0.62448112, 0.49355185]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.28012062, 0.34386957, 0.77474482],\n",
       "        [0.74505344, 0.44301294, 0.04301606]],\n",
       "\n",
       "       [[0.93565316, 0.12371532, 0.7058655 ],\n",
       "        [0.76775782, 0.38293313, 0.89692612]],\n",
       "\n",
       "       [[0.05606994, 0.19828843, 0.70035897],\n",
       "        [0.99030709, 0.41207613, 0.1312103 ]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "data_dim = 3\n",
    "timesteps = 2\n",
    "num_classes = 10\n",
    "\n",
    "x_train = np.random.random((1000, timesteps, data_dim))\n",
    "y_train = np.random.random((1000, num_classes))\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3000\n",
      "23/23 [==============================] - 1s 42ms/step - loss: 0.2654 - acc: 0.0000e+00 - mean_squared_error: 0.2654\n",
      "Epoch 2/3000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2606 - acc: 0.0000e+00 - mean_squared_error: 0.2606\n",
      "Epoch 3/3000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2565 - acc: 0.0000e+00 - mean_squared_error: 0.2565\n",
      "Epoch 4/3000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2526 - acc: 0.0000e+00 - mean_squared_error: 0.2526\n",
      "Epoch 5/3000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2490 - acc: 0.0000e+00 - mean_squared_error: 0.2490\n",
      "Epoch 6/3000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/keras/callbacks.py:535: RuntimeWarning: Early stopping conditioned on metric `val_loss` which is not available. Available metrics are: mean_squared_error,acc,loss\n",
      "  (self.monitor, ','.join(list(logs.keys()))), RuntimeWarning\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2456 - acc: 0.0000e+00 - mean_squared_error: 0.2456\n",
      "Epoch 7/3000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2424 - acc: 0.0000e+00 - mean_squared_error: 0.2424\n",
      "Epoch 8/3000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2393 - acc: 0.0000e+00 - mean_squared_error: 0.2393\n",
      "Epoch 9/3000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2364 - acc: 0.0000e+00 - mean_squared_error: 0.2364\n",
      "Epoch 10/3000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2335 - acc: 0.0000e+00 - mean_squared_error: 0.2335\n",
      "Epoch 11/3000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2306 - acc: 0.0000e+00 - mean_squared_error: 0.2306\n",
      "Epoch 12/3000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2279 - acc: 0.0000e+00 - mean_squared_error: 0.2279\n",
      "Epoch 13/3000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2251 - acc: 0.0000e+00 - mean_squared_error: 0.2251\n",
      "Epoch 14/3000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2224 - acc: 0.0000e+00 - mean_squared_error: 0.2224\n",
      "Epoch 15/3000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2198 - acc: 0.0000e+00 - mean_squared_error: 0.2198\n",
      "Epoch 16/3000\n",
      "23/23 [==============================] - 0s 2ms/step - loss: 0.2171 - acc: 0.0000e+00 - mean_squared_error: 0.2171\n",
      "Epoch 17/3000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2144 - acc: 0.0000e+00 - mean_squared_error: 0.2144\n",
      "Epoch 18/3000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2117 - acc: 0.0000e+00 - mean_squared_error: 0.2117\n",
      "Epoch 19/3000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2090 - acc: 0.0000e+00 - mean_squared_error: 0.2090\n",
      "Epoch 20/3000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2063 - acc: 0.0000e+00 - mean_squared_error: 0.2063\n",
      "Epoch 21/3000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2037 - acc: 0.0000e+00 - mean_squared_error: 0.2037\n",
      "Epoch 22/3000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.2010 - acc: 0.0000e+00 - mean_squared_error: 0.2010\n",
      "Epoch 23/3000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1983 - acc: 0.0000e+00 - mean_squared_error: 0.1983\n",
      "Epoch 24/3000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1956 - acc: 0.0000e+00 - mean_squared_error: 0.1956\n",
      "Epoch 25/3000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1928 - acc: 0.0000e+00 - mean_squared_error: 0.1928\n",
      "Epoch 26/3000\n",
      "23/23 [==============================] - 0s 3ms/step - loss: 0.1901 - acc: 0.0000e+00 - mean_squared_error: 0.1901\n",
      "Epoch 27/3000\n",
      "16/23 [===================>..........] - ETA: 0s - loss: 0.1829 - acc: 0.0000e+00 - mean_squared_error: 0.1829"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-7d862010baa0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# fit the model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mlstm_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfit_lstm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scaled\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3000\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0;31m# forecast the entire training dataset to build up state for forecasting\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_reshaped\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_scaled\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_scaled\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# lstm_model.predict(train_reshaped, batch_size=1)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d00704f76d4c>\u001b[0m in \u001b[0;36mfit_lstm\u001b[0;34m(train, batch_size, nb_epoch, neurons)\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;31m#     while earlystop.stopped_epoch == 0:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m     model.fit(X, y, epochs=nb_epoch, batch_size=batch_size, verbose=1, shuffle=False,  \n\u001b[0;32m---> 51\u001b[0;31m               callbacks=callbacks)\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;31m#         print(\"epoch\", epoch,  model.history.history.get(\"loss\")[-1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;31m#         epoch += 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[1;32m   1040\u001b[0m                                         \u001b[0minitial_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitial_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1041\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1042\u001b[0;31m                                         validation_steps=validation_steps)\n\u001b[0m\u001b[1;32m   1043\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1044\u001b[0m     def evaluate(self, x=None, y=None,\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/engine/training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[0;34m(model, f, ins, out_labels, batch_size, epochs, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch, steps_per_epoch, validation_steps)\u001b[0m\n\u001b[1;32m    197\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 199\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    200\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    201\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2659\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_legacy_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2660\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2661\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2662\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2663\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mpy_any\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   2629\u001b[0m                                 \u001b[0msymbol_vals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2630\u001b[0m                                 session)\n\u001b[0;32m-> 2631\u001b[0;31m         \u001b[0mfetched\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_callable_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0marray_vals\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2632\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mfetched\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2633\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1380\u001b[0m           ret = tf_session.TF_SessionRunCallable(\n\u001b[1;32m   1381\u001b[0m               \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_session\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_handle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstatus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1382\u001b[0;31m               run_metadata_ptr)\n\u001b[0m\u001b[1;32m   1383\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1384\u001b[0m           \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# fit the model\n",
    "lstm_model = fit_lstm(train_scaled, 1, 3000, 4)\n",
    "# forecast the entire training dataset to build up state for forecasting\n",
    "train_reshaped = train_scaled[:, 0].reshape(len(train_scaled), 1, 1)\n",
    "# lstm_model.predict(train_reshaped, batch_size=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.0,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " 0.043478260869565216,\n",
       " ...]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_model.history.history.get(\"acc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Month=1, Predicted=283.254168, Expected=339.700000\n",
      "Month=2, Predicted=250.909539, Expected=440.400000\n",
      "Month=3, Predicted=343.823845, Expected=315.900000\n",
      "Month=4, Predicted=129.941431, Expected=439.300000\n",
      "Month=5, Predicted=268.620053, Expected=401.300000\n",
      "Month=6, Predicted=216.183576, Expected=437.400000\n",
      "Month=7, Predicted=268.840729, Expected=575.500000\n",
      "Month=8, Predicted=465.126026, Expected=407.600000\n",
      "Month=9, Predicted=243.417638, Expected=682.000000\n",
      "Month=10, Predicted=683.967605, Expected=475.300000\n",
      "Month=11, Predicted=409.241285, Expected=581.300000\n",
      "Month=12, Predicted=572.481560, Expected=646.900000\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# walk-forward validation on the test data\n",
    "predictions = list()\n",
    "for i in range(len(test_scaled)):\n",
    "    # make one-step forecast\n",
    "    X, y = test_scaled[i, 0:-1], test_scaled[i, -1]\n",
    "    yhat = forecast_lstm(lstm_model, 1, X)\n",
    "    # invert scaling\n",
    "    yhat = invert_scale(scaler, X, yhat)\n",
    "    # invert differencing\n",
    "    yhat = inverse_difference(raw_values, yhat, len(test_scaled)+1-i)\n",
    "    # store forecast\n",
    "    predictions.append(yhat)\n",
    "    expected = raw_values[len(train) + i + 1]\n",
    "    print('Month=%d, Predicted=%f, Expected=%f' % (i+1, yhat, expected))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 218.041\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# report performance\n",
    "rmse = sqrt(mean_squared_error(raw_values[-12:], predictions))\n",
    "print('Test RMSE: %.3f' % rmse)\n",
    "# line plot of observed vs predicted\n",
    "pyplot.plot(raw_values[-12:])\n",
    "pyplot.plot(predictions)\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAX0AAAD8CAYAAACb4nSYAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDIuMi4yLCBodHRwOi8vbWF0cGxvdGxpYi5vcmcvhp/UCwAAEStJREFUeJzt3X+snmddx/H31x7XKdNu6+oc7copaZWUH/LjWFjExDAZHUGKsUtaSWi0SaMyxR9EuxgHLPzhjKFiGIbGTZZGabGgnsxioxv/+KvsFBBWtsphG64T5GyrJYN0W+HrH891xsPjGec57XN+Pd/3KznpfV/39ZxzXbmbz7nPdV33fUdmIkmq4fsWuwGSpIVj6EtSIYa+JBVi6EtSIYa+JBVi6EtSIYa+JBVi6EtSIYa+JBUystgN6HXFFVfk6OjoYjdDkpaV48ePP5aZa2art+RCf3R0lImJicVuhiQtKxHx5X7qObwjSYUY+pJUiKEvSYUY+pJUiKEvSYUY+pJUiKEvSYUsuXX6C+3sM9/iw//6MN986txiN0VScT/2oz/Em172/Hn9GeVD/9NfPs0ffuIBACIWuTGSSnvTy55v6M+3b7UXw3/sV6/hVS+4fJFbI0nzq/yYfst8SSqhfOhLUiWG/rMc0Jc0/MqHvqM7kiopH/qSVImh37hcU1IF5UM/Xb4jqZDyoT/NC31JFZQPfa/zJVVSPvQlqZK+Qj8itkbEyYiYjIi9MxxfGRGH2vFjETHac3x9RDwZEe8cTLMHL5zJlVTArKEfESuA24Drgc3AzojY3FNtN3A6MzcC+4Bbe46/D/jEhTd3Hji+I6mQfq70twCTmflgZj4NHAS29dTZBtzZtg8D10a7dI6ItwAPAScG02RJ0vnqJ/TXAo907Z9qZTPWycxzwBlgdURcAvwe8J4Lb+r8cnBHUgXzPZH7bmBfZj75vSpFxJ6ImIiIiampqXlu0ndLx3ckFdLP8/QfBa7u2l/XymaqcyoiRoBVwOPAq4HtEfFHwKXAtyPibGZ+oPvDmbkf2A8wNja2KCnsPK6kCvoJ/XuBTRGxgU647wB+safOOLAL+DdgO3BPdm51/enpChHxbuDJ3sBfbN6QK6mSWUM/M89FxI3AUWAFcEdmnoiIW4CJzBwHbgcORMQk8ASdXwySpCWmr9clZuYR4EhP2c1d22eBG2b5Hu8+j/YtmHAqV1IB5e/IdXhHUiXlQ1+SKjH0G1fvSKqgfOg7uiOpkvKhL0mVlA9935wlqZLyoS9JlRj6jRO5kiooH/oO7kiqpHzoS1Ilhn7jYxgkVVA+9F28I6mS8qE/zYlcSRUY+k7lSirE0JekQgz9xuEdSRWUD30nciVVUj70JakSQ79xnb6kCsqHvqM7kiopH/rTnMiVVEH50HciV1Il5UNfkiox9BtHdyRVUD7006lcSYWUD31JqsTQb1y9I6mC8qHv6h1JlZQP/e/wUl/S8Csf+l7oS6qkfOhLUiWGfuNErqQKyod+OpMrqZDyoS9JlRj6jaM7kiow9CWpEEO/CWdyJRXQV+hHxNaIOBkRkxGxd4bjKyPiUDt+LCJGW/mWiPhs+/qPiPj5wTb/wjmPK6mSWUM/IlYAtwHXA5uBnRGxuafabuB0Zm4E9gG3tvL7gLHMfDmwFfhQRIwMqvGSpLnp50p/CzCZmQ9m5tPAQWBbT51twJ1t+zBwbUREZn4zM8+18otZwjfAOrgjqYJ+Qn8t8EjX/qlWNmOdFvJngNUAEfHqiDgBfB74la5fAkuCz9OXVMm8T+Rm5rHMfDHwk8BNEXFxb52I2BMRExExMTU1Nd9NkqSy+gn9R4Gru/bXtbIZ67Qx+1XA490VMvN+4EngJb0/IDP3Z+ZYZo6tWbOm/9YPkIt3JFXQT+jfC2yKiA0RcRGwAxjvqTMO7Grb24F7MjPbZ0YAIuIFwIuAhwfS8gFx9Y6kSmZdSZOZ5yLiRuAosAK4IzNPRMQtwERmjgO3AwciYhJ4gs4vBoDXAnsj4hng28CvZeZj89GRCxVO5UoqoK/lk5l5BDjSU3Zz1/ZZ4IYZPncAOHCBbZxXXulLqsQ7ciWpEEO/cSJXUgXlQ9/RHUmVlA99SarE0JekQsqHvq9LlFRJ+dCf5kSupArKh77X+ZIqKR/6klSJod/4ukRJFRj6ju9IKsTQl6RCDP3GwR1JFZQPfV+XKKmS8qE/zXlcSRWUD31vyJVUSfnQl6RKDP3G1yVKqqB86Du6I6mS8qEvSZUY+o2rdyRVUD70Xb0jqZLyoT/NC31JFZQPfe/IlVRJ+dCXpEoM/WmO70gqoHzoO5ErqZLyoS9JlRj6jY9hkFRB+dB3dEdSJeVDf5p35EqqwNB3JldSIYa+JBVi6DeO7kiqoHzoO7gjqZLyoS9JlfQV+hGxNSJORsRkROyd4fjKiDjUjh+LiNFW/vqIOB4Rn2//vm6wzR+ccPmOpAJmDf2IWAHcBlwPbAZ2RsTmnmq7gdOZuRHYB9zayh8Dfi4zXwrsAg4MquGD4uIdSZX0c6W/BZjMzAcz82ngILCtp8424M62fRi4NiIiMz+Tmf/dyk8APxARKwfR8EHzOl9SBf2E/lrgka79U61sxjqZeQ44A6zuqfMLwKcz86nza+r8SC/1JRUyshA/JCJeTGfI57rnOL4H2AOwfv36hWiSJJXUz5X+o8DVXfvrWtmMdSJiBFgFPN721wF/A7wtM7800w/IzP2ZOZaZY2vWrJlbDwbEeVxJFfQT+vcCmyJiQ0RcBOwAxnvqjNOZqAXYDtyTmRkRlwJ/D+zNzH8ZVKMHycEdSZXMGvptjP5G4ChwP/DRzDwREbdExJtbtduB1RExCfw2ML2s80ZgI3BzRHy2ff3IwHshSepLX2P6mXkEONJTdnPX9lnghhk+917gvRfYxgXh8/QlVVD+jlwX70iqpHzoP8sLfUkFlA99L/QlVVI+9CWpEkO/cZ2+pArKh76PYZBUSfnQl6RKDP3G0R1JFRj6klSIod/45ixJFZQPfedxJVVSPvQlqRJDv3FwR1IF5UM/fRCDpELKh74kVVI+9Kcncl28I6mC8qEvSZUY+o1vzpJUQfnQdxpXUiXlQ1+SKjH0GydyJVVQPvR9DIOkSsqHviRVUj70vSNXUiXlQ1+SKjH0GydyJVVQPvSdyJVUSfnQl6RKDP3GxzBIqsDQl6RCDH1JKqR86GebyXX1jqQKyoe+JFVi6Dde6EuqoHzou05fUiXlQ1+SKjH0m3AmV1IBfYV+RGyNiJMRMRkRe2c4vjIiDrXjxyJitJWvjohPRsSTEfGBwTZ9MBzdkVTJrKEfESuA24Drgc3AzojY3FNtN3A6MzcC+4BbW/lZ4A+Adw6sxZKk89bPlf4WYDIzH8zMp4GDwLaeOtuAO9v2YeDaiIjM/EZm/jOd8F+SpidyHdyRVEE/ob8WeKRr/1Qrm7FOZp4DzgCr+21EROyJiImImJiamur3Y5KkOVoSE7mZuT8zxzJzbM2aNYvSBudxJVXQT+g/Clzdtb+ulc1YJyJGgFXA44No4HzzdYmSKukn9O8FNkXEhoi4CNgBjPfUGQd2te3twD2Z3vYkSUvNyGwVMvNcRNwIHAVWAHdk5omIuAWYyMxx4HbgQERMAk/Q+cUAQEQ8DPwwcFFEvAW4LjO/MPiuXBjX6UuqYNbQB8jMI8CRnrKbu7bPAjc8x2dHL6B9886/RyRVsiQmciVJC6N86HuhL6mS8qEvSZUY+rhGX1Idhr4zuZIKMfQlqRBDHx+2JqmO8qHv4I6kSsqHPng3rqQ6yoe+87iSKikf+pJUiaGPE7mS6igf+j5PX1Il5UNfkiox9PExDJLqKB/6rt6RVEn50AcIp3IlFVE+9L3Ql1RJ+dCXpEoMfXChvqQyyoe+E7mSKikf+pJUiaGPozuS6igf+j6GQVIl5UMfvCNXUh2Gvhf6kgox9CWpEEMfH8MgqY7yoe/ojqRKyoe+JFVi6OPqHUl1lA/99DkMkgopH/rgHbmS6igf+l7oS6qkfOhLUiWGPhDO5Eoqoq/Qj4itEXEyIiYjYu8Mx1dGxKF2/FhEjHYdu6mVn4yINwyu6YPh6I6kSmYN/YhYAdwGXA9sBnZGxOaearuB05m5EdgH3No+uxnYAbwY2Ap8sH0/SdIi6OdKfwswmZkPZubTwEFgW0+dbcCdbfswcG10xky2AQcz86nMfAiYbN9vSXFwR1IVI33UWQs80rV/Cnj1c9XJzHMRcQZY3cr/veeza8+7td/DA1/9Or/+V5+Z8+emnnxqHlojSUtTP6E/7yJiD7AHYP369ef1PS4eWcGmKy+Z8+c2XXkJL1176Xn9TElabvoJ/UeBq7v217WymeqciogRYBXweJ+fJTP3A/sBxsbGzmtudfSK5/HBt77qfD4qSWX0M6Z/L7ApIjZExEV0JmbHe+qMA7va9nbgnuw832Ac2NFW92wANgGfGkzTJUlzNeuVfhujvxE4CqwA7sjMExFxCzCRmePA7cCBiJgEnqDzi4FW76PAF4BzwNsz81vz1BdJ0ixiqT1wbGxsLCcmJha7GZK0rETE8cwcm62ed+RKUiGGviQVYuhLUiGGviQVYuhLUiFLbvVOREwBX76Ab3EF8NiAmrOU2c/hYj+Hy2L08wWZuWa2Sksu9C9UREz0s2xpubOfw8V+Dpel3E+HdySpEENfkgoZxtDfv9gNWCD2c7jYz+GyZPs5dGP6kqTnNoxX+pKk5zA0oT/by9uXk4i4OiI+GRFfiIgTEfGOVn55RPxjRHyx/XtZK4+I+NPW989FxCsXtwdzExErIuIzEXFX298QEcdafw61R3rTHtF9qJUfi4jRxWz3XETEpRFxOCIeiIj7I+KaYTyfEfFb7f/sfRHxkYi4eFjOZ0TcERFfi4j7usrmfA4jYler/8WI2DXTz5pPQxH6fb68fTk5B/xOZm4GXgO8vfVnL3B3Zm4C7m770On3pva1B/izhW/yBXkHcH/X/q3AvszcCJwGdrfy3cDpVr6v1Vsu3g/8Q2a+CPgJOv0dqvMZEWuB3wDGMvMldB7FvoPhOZ8fBrb2lM3pHEbE5cC76LxydgvwrulfFAsmM5f9F3ANcLRr/ybgpsVu1wD793fA64GTwFWt7CrgZNv+ELCzq/6z9Zb6F523qd0NvA64i8576h8DRnrPLZ13OlzTtkdavVjsPvTRx1XAQ71tHbbzyXfelX15Oz93AW8YpvMJjAL3ne85BHYCH+oq/656C/E1FFf6zPzy9nl5AftCa3/yvgI4BlyZmV9ph74KXNm2l3P//wT4XeDbbX818L+Zea7td/fl2X6242da/aVuAzAF/EUbxvrziHgeQ3Y+M/NR4I+B/wK+Quf8HGf4zme3uZ7DRT+3wxL6QykiLgE+BvxmZn69+1h2LhOW9dKriHgT8LXMPL7YbZlnI8ArgT/LzFcA3+A7wwDA0JzPy4BtdH7JPR94Hv9/OGRoLZdzOCyh39cL2JeTiPh+OoH/l5n58Vb8PxFxVTt+FfC1Vr5c+/9TwJsj4mHgIJ0hnvcDl0bE9Ks8u/vybD/b8VXA4wvZ4PN0CjiVmcfa/mE6vwSG7Xz+LPBQZk5l5jPAx+mc42E7n93meg4X/dwOS+j38/L2ZSMigs57h+/PzPd1Hep+Af0uOmP90+VvaysGXgOc6fqTc8nKzJsyc11mjtI5Z/dk5luBTwLbW7Xefk73f3urv+SvrDLzq8AjEfHjrehaOu+NHqrzSWdY5zUR8YPt//B0P4fqfPaY6zk8ClwXEZe1v4yua2ULZ7EnRgY4wfJG4D+BLwG/v9jtucC+vJbOn4mfAz7bvt5IZ7zzbuCLwD8Bl7f6QWf10peAz9NZPbHo/Zhjn38GuKttvxD4FDAJ/DWwspVf3PYn2/EXLna759C/lwMT7Zz+LXDZMJ5P4D3AA8B9wAFg5bCcT+AjdOYqnqHz19vu8zmHwC+3Pk8Cv7TQ/fCOXEkqZFiGdyRJfTD0JakQQ1+SCjH0JakQQ1+SCjH0JakQQ1+SCjH0JamQ/wN/weJB4cYixQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "pyplot.plot(lstm_model.history.history.get(\"acc\"))\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
